{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN은 지금까지 하나의 넷웤이면 얜 두개의 넷웤(GENERATIVE, ADVERSARIAL)\n",
    "# 속이려는자와 감별하려는자의 싸움(MIN과 MAX의 치열한 피튀기는 싸움)\n",
    "# 대표적인 UNSUPERVISED LEARNING\n",
    "# 0.5가 나오면 완벽한 제너레이터\n",
    "\n",
    "# DISCRIMINATOR 는 진짜일수록 1에 가까운숫자를 내밈\n",
    "#  GENERATOR는 가짜를 만드는 놈 (실제 데이터분포와 유사한 데이터분포를 GENERATE하는것)\n",
    "\n",
    "# D의 목적은 진짜를 1, 가짜를 0으로 잘 구분했을떄 D가 최대화 된다\n",
    "# G의 목적은 가짜를 1로 구분했을떄, G가 수학적으로 최소화 된다\n",
    "\n",
    "# COST의 최저치는 Log4가 가장 학습이 잘된 상태(수학적으로 그럼)\n",
    "\n",
    "#실제 분포를 복제함으로써 시뮬레이션할때 좋다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-231e78e925d9>:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps =10000\n",
    "batch_size =128\n",
    "learning_rate= 0.0002\n",
    "# G, D - DNN (256개)\n",
    "image_dim =784\n",
    "gen_hidden_dim=256\n",
    "disc_hidden_dim = 256\n",
    "noise_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def he_init(shape):\n",
    "    return tf.random_normal(shape=shape, stddev=1 / tf.sqrt(shape[0]/2))\n",
    "\n",
    "weights = {\n",
    "    'gen_hidden1': tf.Variable(he_init([noise_dim, gen_hidden_dim])),\n",
    "    'gen_out' : tf.Variable(he_init([gen_hidden_dim, image_dim])),\n",
    "    'disc_hidden1' : tf.Variable(he_init([image_dim, disc_hidden_dim])),\n",
    "    'disc_out' : tf.Variable(he_init([disc_hidden_dim,1]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'gen_hidden1' : tf.Variable(tf.zeros([gen_hidden_dim])),\n",
    "    'gen_out': tf.Variable(tf.zeros([image_dim])),\n",
    "    'disc_hidden1' : tf.Variable(tf.zeros([disc_hidden_dim])),\n",
    "    'disc_out' : tf.Variable(tf.zeros([1]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#망은 cnn을 중간에 껴도되고 시그모이드나 렐루 자유롭게 쓸 수있따. rnn은 안끼는 편\n",
    "def generator(x): #G 의 nn 구성\n",
    "    hidden_layer = tf.matmul(x, weights['gen_hidden1'])\n",
    "    hidden_layer = tf.add(hidden_layer, biases['gen_hidden1'])\n",
    "    hidden_layer = tf.nn.relu(hidden_layer)\n",
    "    output_layer = tf.matmul(hidden_layer, weights['gen_out'])\n",
    "    output_layer = tf.add(output_layer, biases['gen_out'])\n",
    "    output_layer = tf.nn.sigmoid(output_layer)\n",
    "    return output_layer\n",
    "\n",
    "def discriminator(x):\n",
    "    hidden_layer = tf.matmul(x, weights['disc_hidden1'])\n",
    "    hidden_layer = tf.add(hidden_layer, biases['disc_hidden1'])\n",
    "    hidden_layer = tf.nn.relu(hidden_layer)\n",
    "    output_layer = tf.matmul(hidden_layer, weights['disc_out'])\n",
    "    output_layer = tf.add(output_layer, biases['disc_out'])\n",
    "    output_layer = tf.nn.sigmoid(output_layer)\n",
    "    return output_layer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_input = tf.placeholder(tf.float32, shape=[None, noise_dim], name = 'input_noise')\n",
    "disc_input = tf.placeholder(tf.float32, shape=[None, image_dim], name = 'disc_noise')\n",
    "\n",
    "gen_sample = generator(gen_input)\n",
    "disc_real = discriminator(disc_input)\n",
    "disc_fake = discriminator(gen_sample)\n",
    "\n",
    "gen_loss = -tf.reduce_mean(tf.log(disc_fake))\n",
    "disc_loss = -tf.reduce_mean(tf.log(disc_real)+ tf.log(1-disc_fake)) \n",
    "#maximize는 -붙인담에 minimize한것과 동일\n",
    "\n",
    "optimizer_gen = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "optimizer_disc = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "gen_vars = [weights['gen_hidden1'], weights['gen_out'],\n",
    "           biases['gen_hidden1'], biases['gen_out']]\n",
    "disc_vars = [weights['disc_hidden1'], weights['disc_out'],\n",
    "           biases['disc_hidden1'], biases['disc_out']]\n",
    "\n",
    "train_gen = optimizer_gen.minimize(gen_loss, var_list = gen_vars)\n",
    "train_disc = optimizer_disc.minimize(disc_loss, var_list = disc_vars) \n",
    "#두개를 분리 시키기위해서 따로 묶어야 한다\n",
    "#동시에 두개의 네트워크를 학습시켜야되니까 나눠야 하는 번거로움쓰\n",
    "\n",
    "#KL DIVERGENCE 를 통해 LOG4 이후에 COST감소를 확인할 수 있다 D,G가 얼마나 비슷한가를 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 G Loss: 0.66084725 D Loss: 1.4737036\n",
      "Step: 100 G Loss: 4.3877916 D Loss: 0.058852132\n",
      "Step: 200 G Loss: 3.1181262 D Loss: 0.22846541\n",
      "Step: 300 G Loss: 2.5478635 D Loss: 0.30368263\n",
      "Step: 400 G Loss: 2.7490153 D Loss: 0.20249903\n",
      "Step: 500 G Loss: 3.3330235 D Loss: 0.10782687\n",
      "Step: 600 G Loss: 3.5284529 D Loss: 0.068967596\n",
      "Step: 700 G Loss: 3.7051613 D Loss: 0.07452514\n",
      "Step: 800 G Loss: 3.615132 D Loss: 0.05998452\n",
      "Step: 900 G Loss: 3.865469 D Loss: 0.050592747\n",
      "Step: 1000 G Loss: 3.733379 D Loss: 0.06774552\n",
      "Step: 1100 G Loss: 3.5877113 D Loss: 0.06514835\n",
      "Step: 1200 G Loss: 3.9382575 D Loss: 0.047725476\n",
      "Step: 1300 G Loss: 3.6427078 D Loss: 0.072430156\n",
      "Step: 1400 G Loss: 3.817533 D Loss: 0.05119943\n",
      "Step: 1500 G Loss: 3.785926 D Loss: 0.06998883\n",
      "Step: 1600 G Loss: 4.153843 D Loss: 0.055082127\n",
      "Step: 1700 G Loss: 4.2033453 D Loss: 0.06917495\n",
      "Step: 1800 G Loss: 4.017217 D Loss: 0.051693022\n",
      "Step: 1900 G Loss: 4.402749 D Loss: 0.059229188\n",
      "Step: 2000 G Loss: 4.442643 D Loss: 0.051732853\n",
      "Step: 2100 G Loss: 5.1373425 D Loss: 0.048455436\n",
      "Step: 2200 G Loss: 4.8981943 D Loss: 0.023155477\n",
      "Step: 2300 G Loss: 5.0416822 D Loss: 0.03149147\n",
      "Step: 2400 G Loss: 5.3745494 D Loss: 0.025109917\n",
      "Step: 2500 G Loss: 5.319898 D Loss: 0.015760105\n",
      "Step: 2600 G Loss: 5.309698 D Loss: 0.016594598\n",
      "Step: 2700 G Loss: 5.863309 D Loss: 0.009350691\n",
      "Step: 2800 G Loss: 5.563442 D Loss: 0.013084167\n",
      "Step: 2900 G Loss: 5.296426 D Loss: 0.019891305\n",
      "Step: 3000 G Loss: 5.661228 D Loss: 0.007715827\n",
      "Step: 3100 G Loss: 5.933367 D Loss: 0.007916638\n",
      "Step: 3200 G Loss: 5.635683 D Loss: 0.009900849\n",
      "Step: 3300 G Loss: 5.680389 D Loss: 0.007471119\n",
      "Step: 3400 G Loss: 5.6518927 D Loss: 0.01620842\n",
      "Step: 3500 G Loss: 5.0551753 D Loss: 0.029576734\n",
      "Step: 3600 G Loss: 4.617618 D Loss: 0.023225326\n",
      "Step: 3700 G Loss: 4.3562336 D Loss: 0.033659518\n",
      "Step: 3800 G Loss: 3.9936595 D Loss: 0.051733583\n",
      "Step: 3900 G Loss: 4.0415463 D Loss: 0.07726438\n",
      "Step: 4000 G Loss: 4.246359 D Loss: 0.07639723\n",
      "Step: 4100 G Loss: 4.394122 D Loss: 0.10498377\n",
      "Step: 4200 G Loss: 4.5878363 D Loss: 0.08936958\n",
      "Step: 4300 G Loss: 4.4281907 D Loss: 0.061536714\n",
      "Step: 4400 G Loss: 4.365449 D Loss: 0.06114063\n",
      "Step: 4500 G Loss: 3.9972122 D Loss: 0.049939565\n",
      "Step: 4600 G Loss: 4.791844 D Loss: 0.029542705\n",
      "Step: 4700 G Loss: 5.0099125 D Loss: 0.025343496\n",
      "Step: 4800 G Loss: 4.387581 D Loss: 0.049337592\n",
      "Step: 4900 G Loss: 4.6517487 D Loss: 0.03587699\n",
      "Step: 5000 G Loss: 4.702778 D Loss: 0.065877914\n",
      "Step: 5100 G Loss: 4.3357277 D Loss: 0.06119468\n",
      "Step: 5200 G Loss: 5.0901175 D Loss: 0.082770854\n",
      "Step: 5300 G Loss: 4.367977 D Loss: 0.11577754\n",
      "Step: 5400 G Loss: 4.7949343 D Loss: 0.056058493\n",
      "Step: 5500 G Loss: 3.7838154 D Loss: 0.11118476\n",
      "Step: 5600 G Loss: 4.226738 D Loss: 0.1046271\n",
      "Step: 5700 G Loss: 4.94076 D Loss: 0.11886041\n",
      "Step: 5800 G Loss: 4.301532 D Loss: 0.10150289\n",
      "Step: 5900 G Loss: 4.532759 D Loss: 0.13270837\n",
      "Step: 6000 G Loss: 4.132264 D Loss: 0.08946431\n",
      "Step: 6100 G Loss: 3.8331327 D Loss: 0.0873269\n",
      "Step: 6200 G Loss: 4.2273 D Loss: 0.0872554\n",
      "Step: 6300 G Loss: 3.9804475 D Loss: 0.1184105\n",
      "Step: 6400 G Loss: 4.035963 D Loss: 0.068524495\n",
      "Step: 6500 G Loss: 3.9907284 D Loss: 0.07753572\n",
      "Step: 6600 G Loss: 3.9812608 D Loss: 0.13482405\n",
      "Step: 6700 G Loss: 3.9630551 D Loss: 0.09641502\n",
      "Step: 6800 G Loss: 3.6395323 D Loss: 0.11721837\n",
      "Step: 6900 G Loss: 4.015018 D Loss: 0.08499161\n",
      "Step: 7000 G Loss: 3.9310358 D Loss: 0.08166808\n",
      "Step: 7100 G Loss: 3.8474975 D Loss: 0.12881927\n",
      "Step: 7200 G Loss: 3.8140385 D Loss: 0.11757096\n",
      "Step: 7300 G Loss: 3.8324568 D Loss: 0.121892706\n",
      "Step: 7400 G Loss: 3.9270134 D Loss: 0.09275122\n",
      "Step: 7500 G Loss: 3.9111776 D Loss: 0.11840968\n",
      "Step: 7600 G Loss: 4.332279 D Loss: 0.13879701\n",
      "Step: 7700 G Loss: 4.331231 D Loss: 0.087159574\n",
      "Step: 7800 G Loss: 4.1288233 D Loss: 0.13498552\n",
      "Step: 7900 G Loss: 4.120066 D Loss: 0.13338189\n",
      "Step: 8000 G Loss: 3.9602265 D Loss: 0.164848\n",
      "Step: 8100 G Loss: 3.7930017 D Loss: 0.11639555\n",
      "Step: 8200 G Loss: 3.99067 D Loss: 0.12830392\n",
      "Step: 8300 G Loss: 4.090412 D Loss: 0.097701676\n",
      "Step: 8400 G Loss: 4.0359044 D Loss: 0.11676867\n",
      "Step: 8500 G Loss: 3.6508627 D Loss: 0.14449024\n",
      "Step: 8600 G Loss: 4.370141 D Loss: 0.11805788\n",
      "Step: 8700 G Loss: 4.288703 D Loss: 0.13141373\n",
      "Step: 8800 G Loss: 4.439703 D Loss: 0.17034155\n",
      "Step: 8900 G Loss: 4.175729 D Loss: 0.14319637\n",
      "Step: 9000 G Loss: 4.444951 D Loss: 0.27512977\n",
      "Step: 9100 G Loss: 4.101383 D Loss: 0.11920096\n",
      "Step: 9200 G Loss: 3.4576504 D Loss: 0.379225\n",
      "Step: 9300 G Loss: 4.0753293 D Loss: 0.2617507\n",
      "Step: 9400 G Loss: 4.2150908 D Loss: 0.17459497\n",
      "Step: 9500 G Loss: 3.8099613 D Loss: 0.16685599\n",
      "Step: 9600 G Loss: 4.5153794 D Loss: 0.11897195\n",
      "Step: 9700 G Loss: 4.3064365 D Loss: 0.1613958\n",
      "Step: 9800 G Loss: 3.8029366 D Loss: 0.18537092\n",
      "Step: 9900 G Loss: 3.6493692 D Loss: 0.34177348\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(num_steps):\n",
    "    batch_xs, _ = mnist.train.next_batch(batch_size) # disc의 인풋\n",
    "    z = np.random.uniform(-1.,1.,size=[batch_size, noise_dim]) # gen 인풋\n",
    "    _, _, gl, dl = sess.run([train_gen,train_disc,gen_loss, disc_loss],\n",
    "                        feed_dict={disc_input:batch_xs, gen_input:z})\n",
    "    if i % 100 == 0:\n",
    "        print(\"Step:\",i,\"G Loss:\",gl,\"D Loss:\",dl)\n",
    "# Margin Maximize Problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GAN 은 DCGAN을 많이 씀 (https://dev-strender.github.io/articles/2017-07/decan-introduction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
